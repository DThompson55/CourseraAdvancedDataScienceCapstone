{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL on Weather Data\n",
    "\n",
    "This is farily experimental. Here I keep playing with Weather Data, iterating with visualizations and other parts until I have some clean data to work with. In the end only the Daily Data made much sense. Even so, I kept the rest of the data in case I might use it in other projects later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('csvs/1746061.csv', parse_dates = [1], dtype={19: 'str', 30: 'str', 31: 'str', 33: 'str', 34: 'str', 41: 'str', 42: 'str', 43: 'str', 49: 'str', 51: 'str', 58: 'str', 69: 'str', 71: 'str', 75: 'str', 88: 'str', 89: 'str'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1 = pd.read_csv('~/Downloads/1746061.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate Hourly, Daily, Monthly\n",
    "\n",
    "The first step is to separate data, using just the columns required for that row type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "always = [\"DATE\", \"REPORT_TYPE\", \"SOURCE\"]\n",
    "always = [\"DATE\"]\n",
    "\n",
    "monthly = always+[\"AWND\", \"CDSD\", \"CLDD\", \"DSNW\",  \"MonthlyGreatestPrecip\", \"MonthlyGreatestPrecipDate\", \"MonthlyGreatestSnowDepth\", \"MonthlyGreatestSnowDepthDate\", \"MonthlyGreatestSnowfall\", \"MonthlyGreatestSnowfallDate\", \"MonthlyMaxSeaLevelPressureValue\", \"MonthlyMaxSeaLevelPressureValueDate\", \"MonthlyMaxSeaLevelPressureValueTime\", \"MonthlyMaximumTemperature\", \"MonthlyMeanTemperature\", \"MonthlyMinSeaLevelPressureValue\", \"MonthlyMinSeaLevelPressureValueDate\", \"MonthlyMinSeaLevelPressureValueTime\", \"MonthlyMinimumTemperature\", \"MonthlySeaLevelPressure\", \"MonthlyStationPressure\", \"MonthlyTotalLiquidPrecipitation\", \"MonthlyTotalSnowfall\", \"MonthlyWetBulb\", \"NormalsCoolingDegreeDay\", \"NormalsHeatingDegreeDay\"]\n",
    "daily = always+[\"DailyAverageDewPointTemperature\", \"DailyAverageDryBulbTemperature\", \"DailyAverageRelativeHumidity\", \"DailyAverageSeaLevelPressure\", \"DailyAverageStationPressure\", \"DailyAverageWetBulbTemperature\", \"DailyAverageWindSpeed\", \"DailyCoolingDegreeDays\", \"DailyDepartureFromNormalAverageTemperature\", \"DailyHeatingDegreeDays\", \"DailyMaximumDryBulbTemperature\", \"DailyMinimumDryBulbTemperature\", \"DailyPeakWindDirection\", \"DailyPeakWindSpeed\", \"DailyPrecipitation\", \"DailySnowDepth\", \"DailySnowfall\", \"DailySustainedWindDirection\", \"DailySustainedWindSpeed\", \"Sunrise\", \"Sunset\"]\n",
    "daily = always+[\"DailyAverageDryBulbTemperature\", \"DailyPeakWindSpeed\", \"DailyPrecipitation\", \"DailySnowDepth\", \"DailySnowfall\", \"Sunrise\", \"Sunset\"]\n",
    "hourly = always+[\"HourlyAltimeterSetting\", \"HourlyDewPointTemperature\", \"HourlyDryBulbTemperature\", \"HourlyPressureTendency\", \"HourlyRelativeHumidity\", \"HourlySeaLevelPressure\", \"HourlyStationPressure\", \"HourlyVisibility\", \"HourlyWetBulbTemperature\", \"HourlyWindDirection\"]\n",
    "hourly = always+[\"HourlyPrecipitation\", \"HourlyPresentWeatherType\", \"HourlySkyConditions\", \"HourlyWindSpeed\"]\n",
    "drop_cols = [\"STATION\", \"ShortDurationEndDate010\", \"ShortDurationEndDate015\", \"ShortDurationEndDate020\", \"ShortDurationEndDate030\", \"ShortDurationEndDate045\", \"ShortDurationEndDate060\", \"ShortDurationEndDate080\", \"ShortDurationEndDate100\", \"ShortDurationEndDate120\", \"ShortDurationEndDate150\", \"ShortDurationEndDate180\", \"ShortDurationPrecipitationValue005\", \"ShortDurationPrecipitationValue010\", \"ShortDurationPrecipitationValue015\", \"ShortDurationPrecipitationValue020\", \"ShortDurationPrecipitationValue030\", \"ShortDurationPrecipitationValue045\", \"ShortDurationPrecipitationValue060\", \"ShortDurationPrecipitationValue080\", \"ShortDurationPrecipitationValue100\", \"ShortDurationPrecipitationValue120\", \"ShortDurationPrecipitationValue150\", \"ShortDurationPrecipitationValue180\", \"TStorms\", \"WindEquipmentChangeDate\", \"BackupDirection\", \"BackupDistance\", \"BackupDistanceUnit\", \"BackupElements\", \"BackupElevation\", \"BackupElevationUnit\", \"BackupEquipment\", \"BackupLatitude\", \"BackupLongitude\", \"BackupName\"]\n",
    "never = [\"BackupDirection\", \"BackupDistance\", \"BackupDistanceUnit\", \"BackupElements\", \"BackupElevation\", \"BackupElevationUnit\", \"BackupEquipment\", \"BackupLatitude\", \"BackupLongitude\", \"BackupName\"]\n",
    "never = never + drop_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"REPORT_TYPE\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.drop(columns=never, inplace=True)\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_daily = df1.loc[lambda df: df.REPORT_TYPE == \"SOD  \", :][daily]\n",
    "df_16 = df1.loc[lambda df: df.REPORT_TYPE == \"FM-16\", :][hourly]\n",
    "df_15 = df1.loc[lambda df: df.REPORT_TYPE == \"FM-15\", :][hourly]\n",
    "df_hourly = pd.concat([df_15, df_16])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_hourly.HourlyPrecipitation.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hourly Data\n",
    "\n",
    "Try to locate the weather types. Later on we might make a one-hot encoding out of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_hourly[\"x1\"] = df_hourly.HourlyPresentWeatherType.str.replace(\"[^A-Z ]\", \"\", case=True, flags=0, regex=True).apply(lambda x: (list(set(x.split()))) if (isinstance(x, str) ) else np.NaN)\n",
    "df_hourly[\"x1\"] = df_hourly.HourlyPresentWeatherType.str.replace(\"[^A-Z ]\", \"\", case=True, flags=0, regex=True).apply(lambda x: x.split() if (isinstance(x, str) ) else np.NaN)\n",
    "df_hourly[\"x2\"] = df_hourly.x1.apply(lambda x: x[-1] if (( isinstance(x, list) ) ) else np.NaN)\n",
    "\n",
    "print(\"Weather Types are \", (df_hourly[\"x2\"].unique()))\n",
    "\n",
    "#df_hourly[\"x3\"] = df_hourly.x1.apply(lambda x: x[1] if (( isinstance(x, list) and len(x) > 1 ) ) else np.NaN)\n",
    "df_hourly = pd.get_dummies(df_hourly, prefix=None, columns = [\"x2\"])\n",
    "df_hourly.rename(index=str, inplace=True, columns={\"x2_BR\": \"BR\", \"x2_FG\": \"FG\", \"x2_FZ\": \"FZ\", \"x2_HZ\": \"HZ\", \"x2_RA\": \"RA\", \"x2_SN\": \"SN\", \"x2_UP\": \"UP\", \"x2_FU\": \"FU\", \"x2_FZRA\": \"FZRA\"})\n",
    "\n",
    "df_hourly.HourlyPrecipitation = pd.to_numeric(df_hourly.HourlyPrecipitation.str.replace(\"[A-Z]\", \"\", case=False, flags=0, regex=True))\n",
    "df_hourly = df_hourly.fillna(0)\n",
    "\n",
    "\n",
    "#df_hourly[\"x4\"].unique()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hourly = df_hourly.drop(columns=[\"HourlyPresentWeatherType\",\"HourlySkyConditions\",\"x1\" ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hourly.to_csv(\"csvs/hourly_weather_nyc.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daily Data\n",
    "\n",
    "There are a lot of gaps in the daily data, and just some strange special characters. One cleanup routine helps across all of the daily data rollups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(x):\n",
    "    #print(\"A1\",x.unique())\n",
    "    x  = x.str.replace(\"[A-Z]\", \"\", case=False, flags=0, regex=True)\n",
    "    #print(\"A2\",x.unique())\n",
    "    x  = x.str.replace(\"*\", \"\", case=False, flags=0, regex=False)\n",
    "    #print(\"A3\",x.unique())\n",
    "    x  = pd.to_numeric(x)\n",
    "     #x.astype(int)\n",
    "    #print(\"B\",x.unique())\n",
    "  \n",
    "    return x\n",
    "#df_daily[df_daily['DailySnowDepth'].str.match(\"\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "df_daily['DailyAverageDryBulbTemperature2'] = cleanup(df_daily.DailyAverageDryBulbTemperature)\n",
    "df_daily['DailyPeakWindSpeed2'] = cleanup(df_daily.DailyPeakWindSpeed)\n",
    "df_daily['DailyPrecipitation2'] = cleanup(df_daily.DailyPrecipitation)\n",
    "df_daily['DailySnowDepth2'] = cleanup(df_daily.DailySnowDepth)\n",
    "df_daily['DailySnowfall2'] = cleanup(df_daily.DailySnowfall)\n",
    "df_daily = df_daily.fillna(0)\n",
    "\n",
    "df_daily.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wind Speed\n",
    "\n",
    "Wind Speed is broken in many places. 2,237 miles per hour? We'll need to fix that later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_daily['DailyPeakWindSpeed2'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_daily = df_daily.drop(columns=[\"DailyAverageDryBulbTemperature\",\"DailyPeakWindSpeed\",\"DailyPrecipitation\",\"DailySnowDepth\",\"DailySnowfall\" ])\n",
    "\n",
    "df_daily.rename(index=str, inplace=True, columns={\"DailyAverageDryBulbTemperature2\":\"DailyAverageDryBulbTemperature\", \"DailyPeakWindSpeed2\":\"DailyPeakWindSpeed\", \"DailyPrecipitation2\":\"DailyPrecipitation\", \"DailySnowDepth2\":\"DailySnowDepth\", \"DailySnowfall2\":\"DailySnowfall\"})\n",
    "\n",
    "\n",
    "print(\"Cleaned up Daily\")\n",
    "df_daily.to_csv(\"csvs/daily_weather_nyc.csv\")\n",
    "df_daily.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
